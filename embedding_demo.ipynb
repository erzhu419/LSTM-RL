{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T04:33:21.166676Z",
     "start_time": "2024-12-26T04:33:20.003186Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T04:33:21.943787Z",
     "start_time": "2024-12-26T04:33:21.171343Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('default of credit card clients.xls', header=1)\n",
    "cont_cols = ['LIMIT_BAL','AGE','BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6',\t'PAY_AMT1',\t'PAY_AMT2',\t'PAY_AMT3',\t'PAY_AMT4',\t'PAY_AMT5',\t'PAY_AMT6']\n",
    "cat_cols = ['SEX','EDUCATION','MARRIAGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']\n",
    "target_col = 'default payment next month'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T04:33:22.011686Z",
     "start_time": "2024-12-26T04:33:22.000994Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_split(df):\n",
    "    test_size = 0.3\n",
    "    random_state = 1234\n",
    "    df_train,df_test = train_test_split(df, test_size=test_size, random_state = random_state, stratify =  df[target_col])\n",
    "    return df_train,df_test\n",
    "\n",
    "df_train, df_test = data_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T04:33:22.054833Z",
     "start_time": "2024-12-26T04:33:22.048751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PAY_5', 'PAY_6']\n"
     ]
    }
   ],
   "source": [
    "# Check if all levels in categorical features are present in training set\n",
    "def check_col(col_name):\n",
    "  ref = list(df[col_name].unique())\n",
    "  tar_col = list(df_train[col_name].unique())\n",
    "  chk = [elem for elem in ref if elem not in tar_col]\n",
    "  return chk\n",
    "\n",
    "chk_cols= []\n",
    "for col in cat_cols:\n",
    "  temp_lst = check_col(col)\n",
    "  if len(temp_lst)>0:\n",
    "    chk_cols.append(col)\n",
    "\n",
    "print(chk_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T04:33:22.103776Z",
     "start_time": "2024-12-26T04:33:22.097389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAY_0 {-2: 0, -1: 1, 0: 2, 1: 3, 2: 4, 3: 5, 4: 6, 5: 7, 6: 8, 7: 9, 8: 10}\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from 0 to n-1 for each level in every categorical feature\n",
    "cat_code_dict= {}\n",
    "for col in cat_cols:\n",
    "  temp = df_train[col].astype('category')\n",
    "  cat_code_dict[col] = {val:idx for idx,val in enumerate(temp.cat.categories)}\n",
    "\n",
    "key_name = 3\n",
    "print(cat_cols[key_name],cat_code_dict[cat_cols[key_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T04:33:22.154590Z",
     "start_time": "2024-12-26T04:33:22.151561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SEX': 2, 'EDUCATION': 7, 'MARRIAGE': 4, 'PAY_0': 11, 'PAY_2': 11, 'PAY_3': 11, 'PAY_4': 11, 'PAY_5': 9, 'PAY_6': 9}\n"
     ]
    }
   ],
   "source": [
    "embedding_size_dict = {key: len(val) for key, val in cat_code_dict.items()}\n",
    "\n",
    "print(embedding_size_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T04:33:22.224325Z",
     "start_time": "2024-12-26T04:33:22.220931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SEX': 1, 'EDUCATION': 3, 'MARRIAGE': 2, 'PAY_0': 5, 'PAY_2': 5, 'PAY_3': 5, 'PAY_4': 5, 'PAY_5': 4, 'PAY_6': 4}\n"
     ]
    }
   ],
   "source": [
    "embedding_dim_dict= {key: min(50,val//2) for key,val in embedding_size_dict.items()}\n",
    "\n",
    "print(embedding_dim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T04:33:22.523728Z",
     "start_time": "2024-12-26T04:33:22.267263Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class Process_Dataset:\n",
    "    def __init__(self,path,cat_cols,cont_cols,target_col):\n",
    "        self.cat_cols = cat_cols\n",
    "        self.cont_cols = cont_cols\n",
    "        self.target_col = target_col\n",
    "        self.df= pd.read_excel(path, header=1) ##Load the dataset\n",
    "        self.cat_cols = cat_cols   ##Initialize all categorical features\n",
    "        self.cont_cols = cont_cols  ##Initialize all continuous features\n",
    "        self.target_col = target_col ##Set the target feature\n",
    "        self.data_split()  ##Split the data into train and test set\n",
    "        self._preprocess()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.df_train = self._process(self.df_train,1)\n",
    "        self.df_test = self._process(self.df_test)\n",
    "        self.df_val = self._process(self.df_val)\n",
    "\n",
    "    def data_split(self):\n",
    "        '''\n",
    "        Splits the data into 60% train set , 30% val set and 10% test set\n",
    "        '''\n",
    "        test_size = 0.1\n",
    "        val_size = 0.3\n",
    "        random_state = 1234\n",
    "        self.df_train,self.df_test = train_test_split(self.df, test_size=test_size, random_state = random_state, stratify = self.df[target_col])\n",
    "        self.df_train,self.df_val = train_test_split(self.df_train, test_size= val_size, random_state = random_state, stratify = self.df_train[target_col])\n",
    "\n",
    "    def _preprocess(self):\n",
    "\n",
    "        '''\n",
    "         Creates a mapping from 0 to n-1 for each level in every categorical feature\n",
    "        '''\n",
    "        self.cat_code_dict= {}\n",
    "        for col in self.cat_cols:\n",
    "            temp = self.df_train[col].astype('category')\n",
    "            self.cat_code_dict[col] = {val:idx for idx,val in enumerate(temp.cat.categories)}\n",
    "\n",
    "\n",
    "    def _process(self,_df, flag=0):\n",
    "        '''\n",
    "        We scale numerical variables using StandardScaler from scikit-learn\n",
    "        '''\n",
    "        _df = _df.copy()\n",
    "        if flag:\n",
    "          self.scaler.fit(_df[cont_cols])\n",
    "\n",
    "        # numeric fields\n",
    "        _df[self.cont_cols] = self.scaler.transform(_df[cont_cols])\n",
    "        _df[self.cont_cols] = _df[self.cont_cols].astype(np.float32)\n",
    "\n",
    "        # categorical fields\n",
    "        for col in self.cat_cols:\n",
    "            code_dict = self.cat_code_dict[col]\n",
    "            _df[col] = _df[col].map(code_dict).astype(np.int64)\n",
    "\n",
    "        # Target\n",
    "        _df[target_col] = _df[self.target_col].astype(np.float32)\n",
    "        return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T04:33:22.542117Z",
     "start_time": "2024-12-26T04:33:22.536337Z"
    }
   },
   "outputs": [],
   "source": [
    "class EntityEmbeddingNN(nn.Module):\n",
    "    def __init__(self,cat_code_dict,cat_cols,cont_cols,target_col,n_classes):\n",
    "        super().__init__()\n",
    "        self.cat_code_dict = cat_code_dict\n",
    "        self.cat_cols = cat_cols   ##Initialize all categorical features\n",
    "        self.cont_cols = cont_cols  ##Initialize all continuous features\n",
    "        self.target_col = target_col ##Set the target feature\n",
    "        self.embeddings = self._create_embedding_vectors()\n",
    "        self.in_features = self.total_embed_dim + len(cont_cols)\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.in_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, n_classes)\n",
    "        )\n",
    "\n",
    "    def _create_embedding_vectors(self):\n",
    "        '''\n",
    "        Create Embedding Layer for each of the categorical variable in dataset\n",
    "        '''\n",
    "        ##Get no of levels in each categorical variable and store in dictionary\n",
    "        self.embedding_size_dict = {key: len(val) for key, val in self.cat_code_dict.items()}\n",
    "        ##Determine dimension of embeddng vector for each categorical variable\n",
    "        self.embedding_dim_dict = { key: min(50, val // 2) for key, val in self.embedding_size_dict.items()}\n",
    "        embeddings = {}\n",
    "        self.total_embed_dim = 0\n",
    "        for col in self.cat_cols:\n",
    "            num_embeddings = self.embedding_size_dict[col]\n",
    "            embedding_dim = self.embedding_dim_dict[col]\n",
    "            embeddings[col] = nn.Embedding(num_embeddings, embedding_dim)\n",
    "            self.total_embed_dim += embedding_dim\n",
    "        return nn.ModuleDict(embeddings)\n",
    "\n",
    "    def forward(self, cat_tensor, num_tensor):\n",
    "        embedding_tensor_group = []\n",
    "        for idx, col in enumerate(self.cat_cols):\n",
    "            layer = self.embeddings[col]\n",
    "            out = layer(cat_tensor[:, idx])\n",
    "            embedding_tensor_group.append(out)\n",
    "\n",
    "            embed_tensor = torch.cat(embedding_tensor_group, dim=1)\n",
    "        out_tensor = torch.cat((embed_tensor, num_tensor), dim=1)\n",
    "        out_tensor = self.layers(out_tensor)\n",
    "\n",
    "        return out_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T04:33:22.591980Z",
     "start_time": "2024-12-26T04:33:22.588244Z"
    }
   },
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, df, cat_cols,cont_cols,target_col):\n",
    "        self.cat_cols = cat_cols\n",
    "        self.cont_cols = cont_cols\n",
    "        self.target_col = target_col\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cat_array = self.df[self.cat_cols].iloc[idx].values\n",
    "        cont_array = self.df[self.cont_cols].iloc[idx].values\n",
    "        target_array = self.df[self.target_col].iloc[idx]\n",
    "        cat_array = torch.LongTensor(cat_array)\n",
    "        cont_array = torch.FloatTensor(cont_array)\n",
    "\n",
    "        return cont_array, cat_array, target_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T04:34:54.220652Z",
     "start_time": "2024-12-26T04:33:22.637284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use 7 categorical features\n",
      "We will use 14 continuous features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:52<00:00,  3.29s/it]\n"
     ]
    }
   ],
   "source": [
    "cont_cols = ['LIMIT_BAL',\t'AGE',\t'BILL_AMT1',\t'BILL_AMT2',\t'BILL_AMT3',\t'BILL_AMT4',\t'BILL_AMT5',\t'BILL_AMT6',\t'PAY_AMT1',\t'PAY_AMT2',\t'PAY_AMT3',\t'PAY_AMT4',\t'PAY_AMT5',\t'PAY_AMT6']\n",
    "cat_cols = ['SEX',\t'EDUCATION',\t'MARRIAGE',\t'PAY_0',\t'PAY_2',\t'PAY_3',\t'PAY_4']\n",
    "target_col = 'default payment next month'\n",
    "\n",
    "print(f\"We will use {len(cat_cols)} categorical features\")\n",
    "print(f\"We will use {len(cont_cols)} continuous features\")\n",
    "\n",
    "\n",
    "###First do all the preprocessing (Scaling and splitting dataset)####\n",
    "dataset = Process_Dataset(\"default of credit card clients.xls\",cat_cols,cont_cols,target_col)\n",
    "##Create train and test instances of Dataset class##\n",
    "dataset_train= TabularDataset(dataset.df_train, cat_cols, cont_cols,target_col)\n",
    "dataset_test= TabularDataset(dataset.df_test, cat_cols, cont_cols,target_col)\n",
    "dataset_val= TabularDataset(dataset.df_val, cat_cols, cont_cols,target_col)\n",
    "##Create train and test dataloaders##\n",
    "train_loader = DataLoader(dataset_train,batch_size=128, num_workers=32,drop_last=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=128, num_workers=32,drop_last=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=128, num_workers=32,drop_last=False)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EntityEmbeddingNN(dataset.cat_code_dict, cat_cols,cont_cols,target_col,1)\n",
    "model= model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "train_loss_per_iter = []\n",
    "train_loss_per_batch = []\n",
    "\n",
    "test_loss_per_iter = []\n",
    "test_loss_per_batch = []\n",
    "# Install tqdm package\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import tqdm\n",
    "\n",
    "n_epochs=16\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    running_loss = 0.0\n",
    "    for idx, (cont_array, cat_array, target_array) in enumerate(train_loader):\n",
    "        cont_array = cont_array.to(device)\n",
    "        cat_array = cat_array.to(device)\n",
    "        target_array = target_array.to(device)\n",
    "\n",
    "        outputs = model(cat_array,cont_array)\n",
    "        loss = F.binary_cross_entropy_with_logits(outputs.squeeze(1),target_array)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        train_loss_per_iter.append(loss.item())\n",
    "\n",
    "\n",
    "    train_loss_per_batch.append(running_loss / (idx + 1))\n",
    "    running_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (cont_array, cat_array, target_array) in enumerate(test_loader):\n",
    "            cont_array = cont_array.to(device)\n",
    "            cat_array = cat_array.to(device)\n",
    "            target_array = target_array.to(device)\n",
    "\n",
    "            outputs = model(cat_array,cont_array)\n",
    "            loss = F.binary_cross_entropy_with_logits(outputs.squeeze(1),target_array)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            test_loss_per_iter.append(loss.item())\n",
    "\n",
    "    test_loss_per_batch.append(running_loss / (idx + 1))\n",
    "    running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T04:34:54.332092Z",
     "start_time": "2024-12-26T04:34:54.327015Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model):\n",
    "    y_pred=[]\n",
    "    y_actual=[]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (cont_array, cat_array, target_array) in enumerate(val_loader):\n",
    "            y_actual.append(target_array)\n",
    "            cont_array = cont_array.to(device)\n",
    "            cat_array = cat_array.to(device)\n",
    "            target_array = target_array.to(device)\n",
    "            outputs = model(cat_array,cont_array)\n",
    "            y_prob = torch.sigmoid(outputs).cpu().numpy()\n",
    "            y_pred.append(y_prob)\n",
    "\n",
    "\n",
    "    y_pred = np.array([elem for ind_list in y_pred for elem in ind_list])\n",
    "    y_actual = np.array([elem for ind_list in y_actual for elem in ind_list])\n",
    "\n",
    "    return y_pred,y_actual\n",
    "\n",
    "def compute_score(y_true, y_pred, round_digits=3):\n",
    "    log_loss = round(metrics.log_loss(y_true, y_pred), round_digits)\n",
    "    auc = round(metrics.roc_auc_score(y_true, y_pred), round_digits)\n",
    "\n",
    "    precision, recall, threshold = metrics.precision_recall_curve(y_true, y_pred)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    mask = ~np.isnan(f1)\n",
    "    f1 = f1[mask]\n",
    "    precision = precision[mask]\n",
    "    recall = recall[mask]\n",
    "\n",
    "    best_index = np.argmax(f1)\n",
    "    threshold = round(threshold[best_index], round_digits)\n",
    "    precision = round(precision[best_index], round_digits)\n",
    "    recall = round(recall[best_index], round_digits)\n",
    "    f1 = round(f1[best_index], round_digits)\n",
    "\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'threshold': threshold,\n",
    "        'log_loss': log_loss\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T04:34:57.928904Z",
     "start_time": "2024-12-26T04:34:54.356724Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred,y_actual = predict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T04:34:57.970218Z",
     "start_time": "2024-12-26T04:34:57.955933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 0.761,\n",
       " 'precision': 0.537,\n",
       " 'recall': 0.53,\n",
       " 'f1': 0.533,\n",
       " 'threshold': 0.276,\n",
       " 'log_loss': 0.444}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_score(y_actual,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSTM-RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
